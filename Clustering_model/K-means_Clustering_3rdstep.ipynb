{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27901,"status":"ok","timestamp":1607752481131,"user":{"displayName":"zahra ghadiri","photoUrl":"","userId":"12753995234969589722"},"user_tz":-210},"id":"5Jgh7JKXX02C","outputId":"b6e8e2af-3bb8-47ad-b8ec-22bfb76efbf9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNsKoEt2Y7p8"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import json\n","import numpy as np\n","import re\n","import random\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","import pickle\n","from scipy.sparse import coo_matrix, hstack\n","\n","from sklearn.metrics.cluster import homogeneity_score\n","from sklearn.metrics.cluster import v_measure_score\n","from sklearn.metrics.cluster import completeness_score\n","\n","from sklearn.decomposition import TruncatedSVD\n","\n","from sklearn.cluster import KMeans, MiniBatchKMeans\n","import datetime\n","from datetime import timedelta\n","\n","from sklearn.metrics import silhouette_samples, silhouette_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5229,"status":"ok","timestamp":1607752491000,"user":{"displayName":"zahra ghadiri","photoUrl":"","userId":"12753995234969589722"},"user_tz":-210},"id":"1TSaqDSPZ8L7","outputId":"5408bcc7-237a-4ccf-9fa9-1f476c5c9276"},"outputs":[{"data":{"text/plain":["(378370, 5)"]},"execution_count":5,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["total=pd.read_json(\"/content/drive/MyDrive/NLP_News/Data/total.json\")\n","total.shape\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"elapsed":1306,"status":"ok","timestamp":1607752493618,"user":{"displayName":"zahra ghadiri","photoUrl":"","userId":"12753995234969589722"},"user_tz":-210},"id":"oQeshRIABzEp","outputId":"a6926100-b3c7-4efc-a852-f20db39cc485"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>date</th>\n","      <th>text</th>\n","      <th>username</th>\n","      <th>ALL</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>679451396844990465</td>\n","      <td>2015-12-23 00:00:11</td>\n","      <td>gulf nations demand release of abducted qatari...</td>\n","      <td>ajenews</td>\n","      <td>[iraq, qataris, gulf nations]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>679451620514611201</td>\n","      <td>2015-12-23 00:01:04</td>\n","      <td>midway through enrollment season for president...</td>\n","      <td>ap</td>\n","      <td>[obama]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>679452844307705856</td>\n","      <td>2015-12-23 00:05:56</td>\n","      <td>russia bombs kill syria civilians</td>\n","      <td>bbcworld</td>\n","      <td>[syria, russia]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>679453367106678785</td>\n","      <td>2015-12-23 00:08:01</td>\n","      <td>six republican candidates expected in fox busi...</td>\n","      <td>cnn</td>\n","      <td>[fox, republican, six]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>679453408324108288</td>\n","      <td>2015-12-23 00:08:10</td>\n","      <td>video bagpipes amid the bustle in tokyo</td>\n","      <td>bbcworld</td>\n","      <td>[tokyo]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   id  ...                            ALL\n","0  679451396844990465  ...  [iraq, qataris, gulf nations]\n","1  679451620514611201  ...                        [obama]\n","2  679452844307705856  ...                [syria, russia]\n","3  679453367106678785  ...         [fox, republican, six]\n","4  679453408324108288  ...                        [tokyo]\n","\n","[5 rows x 5 columns]"]},"execution_count":6,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["total.columns = ['id', 'date','text','username','NE']\n","total.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_9DA6KNIgkJ"},"outputs":[],"source":["import itertools\n","def find_important_vocab(dataset):\n","\n","  NE_list = dataset.NE.to_list()\n","  NE_list = list(itertools.chain(*NE_list))\n","  NE_list = list(np.char.lower(NE_list))\n","  NE_list = list(set(NE_list))\n","\n","  return NE_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5rPlSkkwEQJ"},"outputs":[],"source":["def tf_idf_vec(start,c_time,types):\n","\n","  if types==\"week\":\n","    min_time=total.date[start]\n","  elif types==\"half_week\":\n","    min_time=total.date[start]+datetime.timedelta(days=int(c_time/2))\n","\n","  middle_time=min_time+datetime.timedelta(days=int(c_time/2))\n","  dt = total[(total.date < min_time + datetime.timedelta(days=c_time)) & (total.date >= min_time)]\n","  ind = dt.index\n","  # print(ind)\n","  try:\n","    start =ind[0]\n","    end= ind[-1]\n","    print(\"start\",start)\n","    print(\"end\",end)\n","\n","    important_vocab=find_important_vocab(dt)\n","  \n","    cv = TfidfVectorizer(stop_words=\"english\",vocabulary=important_vocab)\n","    docs_total=dt[\"text\"].tolist()\n","    total_tweets = cv.fit_transform(docs_total)\n","\n","    # print(total_tweets.shape)\n","\n","    return cv,total_tweets,end,start,middle_time\n","\n","  except:\n","    print(\"The Week Is Empty!!!!\")\n","    pass  \n","  return -1,-1,-1,-1,-1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9UP88um0xNE"},"outputs":[],"source":["def km_cluster(total_tweets):\n","\n","  range_n_clusters = np.arange(int(3*total_tweets.shape[0]/8),int(total_tweets.shape[0]/2),22)\n","  scores=[]\n","  for n_clusters in range_n_clusters:\n","    # print(\"n_cluster\",n_clusters)\n","  \n","    km= KMeans(n_clusters=n_clusters, random_state=0)\n","    km_labels = km.fit_predict(total_tweets)\n","\n","    silhouette_avg = silhouette_score(total_tweets, km_labels)\n","    # print(\"score=\",silhouette_avg)\n","    scores.append(silhouette_avg)\n","    if len(scores)>2:\n","      if scores[-1]==scores[-2]:\n","        break\n","\n","  n_clusters=scores.index(max(scores))\n","  print(\"max_score\",max(scores))\n","  n_clusters=range_n_clusters[n_clusters]\n","  km= KMeans(n_clusters=n_clusters, random_state=0)\n","  km_labels = km.fit(total_tweets)\n","  best_model =pickle.dumps(km)\n","\n","  return best_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-g5Ew5mSz5T"},"outputs":[],"source":["def main_func(total,c_time,start_point):\n","  start=start_point\n","  i=1\n","  model_data= pd.DataFrame(columns=('week','middle_time','model','tf_idf', 'start_ind', 'end_ind'))\n","  print(total.shape[0])\n","  while start<=total.shape[0]+start_point:\n","\n","    tf_model,total_tweets,end_ind,start_ind,middle_time=tf_idf_vec(start,c_time,types=\"week\")\n","    cluster=km_cluster(total_tweets)\n","\n","    new_row = {'week':i, 'middle_time':middle_time, 'model':cluster,'tf_idf':pickle.dumps(tf_model),\n","               'start_ind':start_ind,'end_ind':end_ind}\n","    model_data=model_data.append(new_row,ignore_index=True)\n","\n","    tf_model_2,total_tweets_2,end_ind_2,start_ind_2,middle_time_2=tf_idf_vec(start,c_time,types=\"half_week\")\n","    if end_ind_2==-1:\n","      i+1\n","      start=end_ind+1\n","      continue\n","\n","    cluster_2=km_cluster(total_tweets_2)\n","    new_row = {'week':i+0.5, 'middle_time':middle_time_2, 'model':cluster_2,'tf_idf':pickle.dumps(tf_model_2),\n","               'start_ind':start_ind_2,'end_ind':end_ind_2}\n","    model_data=model_data.append(new_row,ignore_index=True)\n","\n","    i+=1\n","    start=end_ind+1\n","\n","  return model_data"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyONu1DU0mag0N/grcW84lXS","collapsed_sections":[],"name":"K-means Clustering (3rd step).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{}},"nbformat":4,"nbformat_minor":0}